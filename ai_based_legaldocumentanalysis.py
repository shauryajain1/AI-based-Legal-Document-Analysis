# -*- coding: utf-8 -*-
"""Ai based LegalDocumentAnalysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aLJv67B-HhtPh23ePTuMj6oET0GFVG2s
"""

import spacy
from transformers import BertTokenizer

# Load SpaCy model for NLP tasks
nlp = spacy.load("en_core_web_sm")

# Load BERT tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

def preprocess_text(text):
    # Tokenize text using SpaCy
    doc = nlp(text)
    # Tokenize text using BERT tokenizer
    tokens = tokenizer.tokenize(text)
    # Convert tokens to input IDs for BERT
    input_ids = tokenizer.convert_tokens_to_ids(tokens)
    return input_ids, tokens

# Example usage
text = "This agreement is made between the Company and the Contractor."
input_ids, tokens = preprocess_text(text)
print("Tokens:", tokens)
print("Input IDs:", input_ids)

import spacy

# Load a pre-trained NER model from SpaCy
nlp = spacy.load("en_core_web_sm")

def extract_entities(text):
    doc = nlp(text)
    entities = []
    for ent in doc.ents:
        entities.append((ent.text, ent.label_))
    return entities

# Example usage
text = "The contractor shall complete the work by December 31, 2024."
entities = extract_entities(text)
print("Entities:", entities)

from transformers import BertForSequenceClassification, Trainer, TrainingArguments

# Load pre-trained BERT model
model = BertForSequenceClassification.from_pretrained('bert-base-uncased')

# Training arguments
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
)

# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,  # Assumes you have prepared your dataset
    eval_dataset=eval_dataset
)

# Fine-tune the model
trainer.train()

from transformers import T5Tokenizer, T5ForConditionalGeneration

# Load T5 model and tokenizer
tokenizer = T5Tokenizer.from_pretrained('t5-small')
model = T5ForConditionalGeneration.from_pretrained('t5-small')

def summarize_text(text):
    # Preprocess the input text
    input_ids = tokenizer.encode("summarize: " + text, return_tensors="pt", max_length=512, truncation=True)
    # Generate summary
    summary_ids = model.generate(input_ids, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    return summary

# Example usage
text = "This is a long legal document that needs to be summarized..."
summary = summarize_text(text)
print("Summary:", summary)

from flask import Flask, request, render_template
import os

app = Flask(__name__)

@app.route('/', methods=['GET', 'POST'])
def upload_file():
    if request.method == 'POST':
        # Get the file from the request
        file = request.files['file']
        # Save the file locally
        file_path = os.path.join("uploads", file.filename)
        file.save(file_path)

        # Read the file and process
        with open(file_path, 'r') as f:
            text = f.read()

        # Process the text (e.g., summarize it)
        summary = summarize_text(text)

        return render_template('result.html', summary=summary)

    return render_template('upload.html')

if __name__ == "__main__":
    app.run(debug=True)

#bash code
AI-Legal-Document-Analyzer/
│
├── app.py                # Flask application
├── data/
│   ├── train/            # Training data
│   └── test/             # Testing data
├── models/
│   └── bert/             # Fine-tuned BERT model
├── templates/
│   ├── upload.html       # HTML form for uploading documents
│   └── result.html       # HTML for displaying the summary
├── static/
│   └── style.css         # CSS for styling the app
├── utils/
│   ├── preprocessing.py  # Preprocessing functions
│   ├── extraction.py     # Entity extraction code
│   └── summarization.py  # Summarization code
└── requirements.txt      # List of dependencies